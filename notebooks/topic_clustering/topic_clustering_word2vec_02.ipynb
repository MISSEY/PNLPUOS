{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import io\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from panns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dataset: 100\n",
      "Example comment:\n",
      "Teach  manners. Better pay less hours. Pay over time for filling in this.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "df = pd.read_csv(\"../../data/comments_testdata_small.csv\")\n",
    "\n",
    "# Clean comments for any unwanted elements. Consider adjusting based on clustering models.\n",
    "for i in range(len(df)):\n",
    "    df.at[i, 'Comment'] = str(df.loc[i]['Comment']).replace('xxxx', '').replace('*', '')\n",
    "    \n",
    "# TODO: Remove stop words.\n",
    "# TODO: Clean for typos.\n",
    "    \n",
    "# Inspect the dataset.\n",
    "print(\"Length dataset: {}\".format(len(df)))\n",
    "print(\"Example comment:\\n{}\".format(df.loc[0]['Comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec general model.\n",
    "vector_path = 'GoogleNewsVectors300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(vector_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean embeddings for each comment.\n",
    "mean_embeddings = []\n",
    "for i in range(len(df)):\n",
    "    tokens = nltk.tokenize.word_tokenize(df.loc[i]['Comment'])\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embeddings.append(model[token])\n",
    "        except KeyError as e:\n",
    "            # Ignore the word if it does not exist.\n",
    "            pass\n",
    "    \n",
    "    mean_embedding = np.array(embeddings).mean(axis=0)\n",
    "    mean_embeddings.append(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.36675553e-02  6.84110224e-02  1.14605241e-02  1.28540039e-01\n",
      " -3.49684507e-02  1.11459587e-02  3.14565795e-03 -7.71953911e-02\n",
      "  3.52125913e-02  8.29045251e-02 -5.19972574e-03  7.56929815e-02\n",
      "  2.99641527e-02  2.06017122e-02 -3.53909992e-02  1.20726362e-01\n",
      "  9.57219079e-02  1.57752400e-03 -3.22547331e-02  5.28141893e-02\n",
      "  5.31146713e-02  3.80483791e-02  3.13509442e-02  4.44825962e-02\n",
      " -1.87518783e-02  9.81577337e-02 -7.18231201e-02  6.06501661e-02\n",
      "  2.42966879e-03  6.43780082e-02 -9.79731604e-02  3.25880796e-02\n",
      " -6.01243228e-02 -4.08750698e-02  2.74118278e-02 -5.27020954e-02\n",
      "  8.92052267e-05 -3.78042385e-02 -1.58409700e-02  9.99380276e-02\n",
      "  6.24812208e-02 -1.04853705e-01  4.15391177e-02 -6.86927214e-02\n",
      " -7.27445185e-02 -3.81892286e-02 -6.31432161e-02  8.48576501e-02\n",
      " -2.76160613e-02 -2.29116576e-03 -6.69086128e-02 -2.00007507e-03\n",
      " -5.25841373e-04  1.39629655e-02  7.67446682e-02  3.29284668e-02\n",
      " -1.27280608e-01 -6.22652508e-02  7.10546039e-03 -9.75341797e-02\n",
      " -5.62368557e-02  6.90137446e-02 -1.28483698e-01 -1.14896335e-01\n",
      "  1.71508789e-02  2.87804240e-03 -1.00092962e-01  6.31009638e-02\n",
      " -1.85781624e-02  1.85546875e-02  4.40955535e-02  3.72449420e-02\n",
      "  6.70565069e-02  5.53260222e-02 -5.29292189e-02 -1.27901524e-01\n",
      "  9.97596160e-02  1.03675254e-01  6.35751560e-02  9.18403044e-02\n",
      " -2.98837516e-02 -3.22758593e-02 -1.11553483e-02  3.52125906e-04\n",
      " -9.64073762e-02 -7.00754002e-02 -1.41695458e-02  6.53451756e-02\n",
      "  5.58418855e-02  5.11615463e-02  1.04783282e-01  6.53076172e-02\n",
      " -3.78136262e-02 -1.64726838e-01 -3.81188020e-02 -1.57996550e-01\n",
      "  4.81590852e-02  7.46650696e-02  1.31788990e-02  2.28905305e-02\n",
      " -9.62477457e-03 -1.71471223e-01 -3.27524021e-02 -2.69634537e-02\n",
      " -4.50016893e-02 -7.76648894e-02 -1.40099162e-02 -3.96446809e-02\n",
      " -2.75644157e-02 -5.92557453e-02  6.22323826e-02  1.81133561e-02\n",
      " -1.82917677e-02 -9.43943933e-02  7.97964260e-02  3.32665071e-02\n",
      "  1.19445801e-01 -4.13161051e-03 -2.76066698e-02  9.24447849e-02\n",
      " -6.41174316e-02  1.22445915e-02 -1.07506387e-01  6.41831607e-02\n",
      " -1.74857657e-02  6.94134086e-02 -4.84900847e-02 -2.43389420e-02\n",
      "  5.12084961e-02  3.46808806e-02  2.83578737e-03 -1.44305885e-01\n",
      " -8.78530666e-02 -3.56844403e-02 -1.71329789e-02 -5.06533124e-02\n",
      " -1.56883821e-02 -4.51002866e-02  9.58956196e-04  1.05989896e-01\n",
      "  1.87330972e-02 -1.07210599e-01  1.31742042e-02 -3.21514420e-02\n",
      " -4.74055372e-02 -2.80949511e-02  1.72400847e-02 -3.44426073e-02\n",
      " -4.08841632e-02 -2.46769823e-02  5.87627701e-02 -6.29835855e-03\n",
      " -8.46135095e-02 -8.81723240e-02 -3.78887467e-02 -3.77948470e-02\n",
      "  2.08223774e-03 -8.23974609e-02 -4.55627441e-02 -6.60306513e-02\n",
      "  3.43909627e-03  3.33956219e-02  6.94627035e-03  3.58698927e-02\n",
      "  4.50955890e-02  3.26130204e-02  7.75803775e-02 -1.18794665e-01\n",
      "  3.75295803e-02  1.64231528e-02 -1.41157880e-01 -4.02738117e-02\n",
      " -2.13928223e-02 -4.70933169e-02 -6.81378320e-02  5.26745133e-02\n",
      "  5.44621376e-03 -7.38572329e-02  1.84983469e-03 -1.47153419e-02\n",
      " -4.64407131e-02 -5.32872118e-02 -2.52544694e-02  3.79450880e-02\n",
      " -4.28067707e-02 -4.72552963e-02 -5.72597794e-02 -4.79548536e-02\n",
      "  1.22121960e-01  3.49308886e-02 -3.99345979e-02  2.84048216e-03\n",
      " -4.94384766e-03 -6.81246258e-03 -1.35751575e-01  6.09318651e-02\n",
      "  5.22554852e-02 -1.13177963e-01 -1.03372429e-02 -3.21890041e-02\n",
      " -5.18132709e-02  8.94493684e-02 -5.90867251e-02  1.07337363e-01\n",
      "  1.17049580e-02 -5.34855761e-02 -9.77372378e-02 -1.57282911e-02\n",
      " -6.54602051e-03 -4.94748615e-02  1.10811088e-02  1.94261987e-02\n",
      "  1.66203431e-03  7.81250000e-02 -7.20120966e-02  6.86129034e-02\n",
      "  6.26502410e-02 -4.92976280e-04 -1.01816028e-01  1.43549992e-02\n",
      " -3.55913639e-02  1.34864217e-02  5.98212034e-02 -8.21439326e-02\n",
      "  1.00219727e-01 -1.50475139e-02  7.68714324e-02  3.71187665e-02\n",
      "  2.29492188e-02 -1.76790673e-02  3.63429151e-02 -1.18952826e-01\n",
      "  1.41225964e-01  3.38746943e-02  4.14381027e-02 -4.98657227e-02\n",
      "  9.77407619e-02 -6.95871189e-02  5.84294228e-03  2.27801986e-02\n",
      "  6.25328645e-02 -1.00869397e-02  2.58695171e-03 -1.61808893e-01\n",
      "  1.05543872e-02 -2.68625114e-02 -5.98520115e-02  7.75522068e-02\n",
      " -6.43380955e-02 -3.58511135e-02  2.55643409e-02  1.31436866e-02\n",
      " -2.42743865e-02  8.36510286e-02  3.75366211e-03 -4.08618636e-02\n",
      "  1.26577523e-02  6.20586686e-02 -7.30825588e-02  3.48275974e-02\n",
      "  4.66191210e-02  1.01529632e-03 -7.13606626e-02  3.93348113e-02\n",
      "  1.70898438e-02  8.20782036e-02 -8.28200113e-03 -8.38000979e-03\n",
      " -4.20485288e-02  3.18022519e-02  7.16881379e-02  1.47216797e-01\n",
      "  5.59269823e-02 -2.36276481e-02  5.80209568e-02 -1.33326603e-02\n",
      " -7.98480660e-02 -9.01911780e-03 -9.17604864e-02 -3.08297966e-02\n",
      "  8.68365914e-02  1.45028923e-02  4.58708555e-02  1.30018383e-01\n",
      "  1.17786117e-02  7.51765296e-02 -6.43592253e-02  5.14291599e-02\n",
      "  5.69223240e-02  1.40230626e-01 -6.64532036e-02  4.97671263e-03\n",
      " -1.26591608e-01 -1.08501725e-02 -1.88927278e-02 -2.27646455e-02\n",
      "  2.74658203e-02 -4.98293377e-02 -1.19241569e-02  1.29582332e-02]\n"
     ]
    }
   ],
   "source": [
    "# Just a quick look at the mean_embeddings to see that they are real.\n",
    "print(mean_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare mean embeddings for visualization.\n",
    "filename = 'mean_embeddings.tsv'\n",
    "with open(filename, 'w', newline='') as f:\n",
    "    for vector in mean_embeddings:\n",
    "        values = []\n",
    "        for value in vector:\n",
    "            values.append(value)\n",
    "    \n",
    "        tsv_output = csv.writer(f, delimiter='\\t')\n",
    "        tsv_output.writerow(values)\n",
    "\n",
    "# Visualize the mean embeddings via TensorFlow Projector.\n",
    "# https://projector.tensorflow.org/\n",
    "# Use t-SNE for approx. 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      "\n",
      " is a racist and discriminating garage from the management to some of the employees therefore it needs to be investigate.\n",
      "Health and safety seems so important to the company yet, we don't have enough running time and are not able to go the toilet!!!!\n",
      "The driver's cabs could be cleaner, and I don't get any information about anything.\n",
      "The only problem I have had in my workplace are with controllers speaking to you like you are not important to the company.\n",
      "1- I do the same job as a metroline driver & probably do it better, why should I get paid less than them? 2- Why should I have to wait 8 years to get higher rate when its 2 years in other companies?\n",
      "Wrong time to hand out survey due to current pay talks. If the company was fair to everyone better responses would apply.  does a lot to look like the correct things are done unfortunately not enough action is done.\n",
      "No fan or A / C in the summer.  competitiveness looking. Dirty bus.\n",
      "RTC's are too tight, no consideration for drivers or passengers. Poorly hourly rate of pay drivers don't care if buses run on time. Should be allowed to take holidays when we want to not when given them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cluster 2:\n",
      "\n",
      "The quality of some buses are very poor. Must of driver believing reporting buses in  is west of time.\n",
      "I feel my department is understaffed for the amount of work we have to do on shift. In addition I feel my supervisors do not utilise us in the most efficient manner.\n",
      "I wish the workplace would be less dirty. I mean buses, garage and sometimes unfortunately even colleagues as we sharing the same cab.\n",
      " Decent pay raise = 5 %.  Career progression programme.  Reduce years of top rate pay for  holders entering the company.\n",
      "Stop implementing pay cuts. Stop cheating on raffle results.\n",
      "I think, on this occasion although there is know a review into the railways as a whole, there has been minimal information about the``franchise\"or any``bidding team\".\n",
      "The diversity in this garage is over the top. I feel I'm stressed out before I go out to drive, because some people take things too seriously or get offended for no reason. I do not like coming to work any more.\n",
      "Shifts are too long - rising to health issues. Headlights are dangerously dim on some vehicles = dangerous. Managers do not listen to drivers who are on the road daily.\n"
     ]
    }
   ],
   "source": [
    "# We already see some clusters from t-SNE visualization. Let's use this to generate clusters we can analyze.\n",
    "# For now we group the clusters manually, using some indices provided by TensorFlow Projector.\n",
    "i_cluster_01 = [23, 97, 63, 58, 59, 54, 67, 49]\n",
    "i_cluster_02 = [27, 85, 60, 32, 14, 82, 36, 99]\n",
    "c_cluster_01 = []\n",
    "c_cluster_02 = []\n",
    "\n",
    "for index in i_cluster_01:\n",
    "    c_cluster_01.append(df.loc[index-1]['Comment'])\n",
    "    \n",
    "for index in i_cluster_02:\n",
    "    c_cluster_02.append(df.loc[index-1]['Comment'])\n",
    "    \n",
    "# Visualize the comments in each group to see if there are apparent semantic similarities.\n",
    "print('Cluster 1:\\n')\n",
    "for comment in c_cluster_01:\n",
    "    print(comment)\n",
    "    \n",
    "print('\\n\\n\\n\\n')\n",
    "\n",
    "print('Cluster 2:\\n')\n",
    "for comment in c_cluster_02:\n",
    "    print(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may see some similarities between the comments. Let's establish semantic labels to try and 'explain' the clusters.\n",
    "v_cluster_01 = []\n",
    "v_cluster_02 = []\n",
    "\n",
    "for index in i_cluster_01:\n",
    "    v_cluster_01.append(mean_embeddings[index-1])\n",
    "    \n",
    "for index in i_cluster_02:\n",
    "    v_cluster_02.append(mean_embeddings[index-1])\n",
    "    \n",
    "# Mean embedding of the mean embeddings.\n",
    "me_cluster_01 = np.array(v_cluster_01).mean(axis=0)\n",
    "me_cluster_02 = np.array(v_cluster_02).mean(axis=0)\n",
    "\n",
    "# TODO: Get KNN for these new mean embeddings. See spotify:annoy, facebook:faiss, panns\n",
    "# TODO: See also: https://www.aclweb.org/anthology/W16-1612.pdf\n",
    "# TODO: Extract cluster labels from the KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Label the comments with mean embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
