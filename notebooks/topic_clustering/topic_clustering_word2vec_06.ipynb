{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import io\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from nearpy import Engine\n",
    "from nearpy.hashes import RandomBinaryProjections\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec news model.\n",
    "vector_path = 'GoogleNewsVectors300.bin'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(vector_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "df = pd.read_csv(\"../../data/pnlp_data_en.csv\", delimiter=';')\n",
    "\n",
    "# Clean comments for any unwanted elements. Consider adjusting based on clustering goals.\n",
    "comments_cleaned = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordlist_en = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "for i in range(len(df)):\n",
    "    tokens = nltk.tokenize.word_tokenize(df.loc[i]['Comments'])\n",
    "    tokens_filtered = [w for w in tokens if w in wordlist_en and not w in stop_words]\n",
    "    if len(tokens_filtered) > 1:\n",
    "        comments_cleaned.append(str(df.loc[i]['Comments']).replace('xxxx', '').replace('*', '').lower()) \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Inspect the dataset.\n",
    "print(\"Length dataset: {}\".format(len(comments_cleaned)))\n",
    "print(\"Example comment: {}\".format(comments_cleaned[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new data structure to keep things cleaner.\n",
    "comments = comments_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean embeddings for each comment.\n",
    "mean_embeddings = []\n",
    "for i in range(len(comments)):\n",
    "    tokens = nltk.tokenize.word_tokenize(comments[i])\n",
    "    tokens_filtered = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    embeddings = []\n",
    "    for token in tokens_filtered:\n",
    "        try:\n",
    "            embeddings.append(model[token])\n",
    "        except KeyError as e:\n",
    "            # Ignore the word if it does not exist.\n",
    "            pass\n",
    "    \n",
    "    mean_embedding = np.array(embeddings).mean(axis=0)\n",
    "    mean_embeddings.append(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a clustering algorithm on the data given the number of clusters we expect to find.\n",
    "n_clusters = 12\n",
    "ward = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "        linkage='ward').fit(mean_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the data.\n",
    "cluster_labels = ward.labels_\n",
    "merged = []\n",
    "for i in range(len(comments)):\n",
    "    tpl = (comments[i], mean_embeddings[i], cluster_labels[i])\n",
    "    merged.append(tpl)\n",
    "    \n",
    "df_merged = pd.DataFrame(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to store the cluster data.\n",
    "class Cluster:\n",
    "    def __init__(self, c_cl=None, me_cl=None, i_cl=None, l_cl=None):\n",
    "        self.c_cl = c_cl\n",
    "        self.me_cl = me_cl\n",
    "        self.l_cl =  l_cl\n",
    "        self.i_cl = i_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of cluster data objects.\n",
    "clusters = []\n",
    "for i in range(n_clusters):\n",
    "    c = df_merged[0][df_merged[2] == i]\n",
    "    me = df_merged[1][df_merged[2] == i]    \n",
    "    cluster = Cluster(c_cl=c, me_cl=me, i_cl=i)\n",
    "    clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will place the mean cluster embeddings in the Word2Vec vector space. First prep the NearPy embedding query engine.\n",
    "dimension = 300\n",
    "rbp = RandomBinaryProjections('rbp', 10)\n",
    "engine = Engine(dimension, lshashes=[rbp])\n",
    "\n",
    "# Create list of all valid English words to filter out strange tokens in Word2Vec model.\n",
    "wordlist_en = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "\n",
    "# Build the engine using the Word2Vec model.\n",
    "for word in wordlist_en:\n",
    "    try:\n",
    "        v = model[word]\n",
    "        engine.store_vector(v, word)\n",
    "    except KeyError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using normal words to assess how well it works.\n",
    "test_fruit = engine.neighbours(model['apple'])\n",
    "test_man = engine.neighbours(model['man'])\n",
    "\n",
    "print([i[1] for i in test_fruit])\n",
    "print([i[1] for i in test_man])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add meaningful lables to the clusters.\n",
    "for cluster in clusters:\n",
    "    cluster.l_cl = engine.neighbours(np.array(cluster.me_cl).mean(axis=0))\n",
    "    labels_cleaned = []\n",
    "    for i in cluster.l_cl:\n",
    "        labels_cleaned.append(i[1])\n",
    "        \n",
    "    cluster.l_cl = labels_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the comment clusters and proposed labels.\n",
    "for cluster in clusters:\n",
    "    print(\"Cluster \" + str(cluster.i_cl+1) + \":\", end='')\n",
    "    print(\"{}, {}, {}\".format(cluster.l_cl[0], cluster.l_cl[1], cluster.l_cl[2]))\n",
    "    print('\\n' + \"Sample Comment: \", end='')\n",
    "    print([comment for comment in cluster.c_cl][0])\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare mean embeddings for visualization.\n",
    "filedata = 'mean_embeddings_full_clustered.tsv'\n",
    "filelabels = 'mean_embeddings_full_clustered_labels.tsv'\n",
    "with open(filedata, 'w', newline='', encoding='utf-8') as f1:\n",
    "    tsv_output1 = csv.writer(f1, delimiter='\\t')\n",
    "    with open(filelabels, 'a', newline='', encoding='utf-8') as f2:\n",
    "        tsv_output2 = csv.writer(f2, delimiter='\\t')\n",
    "        tsv_output2.writerow(['comment', 'label'])\n",
    "        for cluster in clusters:\n",
    "            for i in range(len(cluster.c_cl)):\n",
    "                comment = [comment for comment in cluster.c_cl][i]\n",
    "                label = \"{}, {}, {}\".format(cluster.l_cl[0], cluster.l_cl[1], cluster.l_cl[2])\n",
    "                metadata = [comment, label]\n",
    "                me_values = [me for me in cluster.me_cl][i]\n",
    "                \n",
    "                tsv_output1.writerow(me_values)\n",
    "                tsv_output2.writerow(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean embeddings via TensorFlow Projector.\n",
    "\n",
    "# https://projector.tensorflow.org/\n",
    "# Load both the embeddings and labels .tsv files.\n",
    "# Color by label.\n",
    "# Use UMAP visualization.\n",
    "# Play around with other visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Study data more carefully to get a sense of the kinds of clusters we can expect. Eg. 'pay', 'hiring', 'facilities.'\n",
    "# TODO: Clean comments for typos.\n",
    "# TODO: Break longer comments into phrases because topics can be mixed.\n",
    "# TODO: Identify key words to focus the analysis. Prioritize words such as 'pay', 'hiring', etc.\n",
    "# TODO: Cluster comments around vectors of keyword categories.\n",
    "# TODO: Optimize clustering and labeling methods.\n",
    "# TODO: Optimize all methods to get better final results.\n",
    "# TODO: Dynamically identify optimal number of n_clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
