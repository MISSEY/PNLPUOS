{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haven't installed all the nltk assets required for all functions used in this notebook,\n",
    "# just uncomment all the lines in this cell and run it once\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string, random\n",
    "\n",
    "# global variable for english stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in dataset and structure it in a way that is beneficial for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# currently reading in full dataset; small batch of data also available\n",
    "# path when running notebook via VS Code\n",
    "#df = pd.read_csv(\"data/pnlp_data_en.csv\", sep=\";\")\n",
    "\n",
    "# path when running notebook via jupyter\n",
    "df = pd.read_csv(\"../../data/pnlp_data_en.csv\", sep=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up dataframe into individual sets to continue processing\n",
    "large_department = df[df['Report Grouping'] == 'Large Department']\n",
    "ld_q1 = large_department[large_department['Question Text'] == 'Please tell us what is working well.']\n",
    "ld_q2 = large_department[large_department['Question Text'] == 'Please tell us what needs to be improved.']\n",
    "\n",
    "small_department = df[df['Report Grouping'] == 'Small Department']\n",
    "sd_q1 = small_department[small_department['Question Text'] == 'Please tell us what is working well.']\n",
    "sd_q2 = small_department[small_department['Question Text'] == 'Please tell us what needs to be improved.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Function for Data-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens):\n",
    "    \"\"\"\n",
    "    Removes irrelevant information. Furthermore lemmatizes tokens and performs stopword elimination.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # remove hyperlinks (specific to twitter dataset in case we want to do transfer learning)\n",
    "        # token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "        #               '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        # remove mentions of users (also specific to twitter)\n",
    "        # token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        # simplify POS tags for lemmatizer\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        # perform stopword elimination, punctuation removal\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "        # @TODO: implement spell correction?\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for POS Tagging\n",
    "This was written in a context that was abandoned. I will leave the code here in case I ever need to circle back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(df, verbose=False, limit=-1):\n",
    "    \"\"\"\n",
    "    Extracts POS tags for every answer (important: one answer may contain multiple sentences) in a given dataframe and returns them as a list of lists.\n",
    "    Note: Function is specified for df structure of complete dataset provided by deepsight. Index adjustments might be necessary on different dfs.\n",
    "\n",
    "    df: The dataframe containing the sentences to be tagged.\n",
    "    verbose: Should the function report progress? False by default.\n",
    "    limit: If only the first n sents should be tagged, give n as limit. Tags entire df by default.\n",
    "    \"\"\"\n",
    "    # list used to store the individual lists of POS tags\n",
    "    pos_tags = []\n",
    "\n",
    "    # running index used for limit and state updates\n",
    "    i = 0\n",
    "    # set variable encoding amount of total answers to tag\n",
    "    if limit == -1:\n",
    "        end = len(df.index)\n",
    "    else:\n",
    "        end = limit\n",
    "\n",
    "    # iterature over entire dataframe\n",
    "    for row in df.iterrows():\n",
    "        # list used to store POS tags of any given answer\n",
    "        temp = []\n",
    "        # iterate over given answer\n",
    "        # adjust here for different df structures!\n",
    "        for token in nlp(row[1][2]):\n",
    "            temp.append(token.pos_)\n",
    "        # add new tags to complete list of tags\n",
    "        pos_tags.append(temp)\n",
    "\n",
    "        # defines output of verbose run of function\n",
    "        if verbose:\n",
    "            print(\"Tagging answer\", i+1, \"of\", end)\n",
    "\n",
    "        # stops function when limit is reached \n",
    "        if limit != -1:\n",
    "            if i == limit - 1: return(pos_tags)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the POS tag function\n",
    "taglist = pos_tag(ld_q1, verbose=True, limit=10)\n",
    "print(ld_q1['Comments'][0])\n",
    "print(taglist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function for Lemmatization\n",
    "This was written in a context that was abandoned. I will leave the code here in case I ever need to circle back to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df, verbose=False, limit=-1):\n",
    "    \"\"\"\n",
    "    Extracts lemmas for every answer (important: one answer may contain multiple sentences) in a given dataframe and returns them as a list of lists.\n",
    "    Note: Function is specified for df structure of complete dataset provided by deepsight. Index adjustments might be necessary on different dfs.\n",
    "\n",
    "    df: The dataframe containing the sentences to be lemmatized.\n",
    "    verbose: Should the function report progress? False by default.\n",
    "    limit: If only the first n sents should be lemmatized, give n as limit. Lemmatizes entire df by default.\n",
    "    \"\"\"\n",
    "    # list used to store the individual lists of POS tags\n",
    "    lemmas = []\n",
    "\n",
    "    # running index used for limit and state updates\n",
    "    i = 0\n",
    "    # set variable encoding amount of total answers to tag\n",
    "    if limit == -1:\n",
    "        end = len(df.index)\n",
    "    else:\n",
    "        end = limit\n",
    "\n",
    "    # iterature over entire dataframe\n",
    "    for row in df.iterrows():\n",
    "        # list used to store POS tags of any given answer\n",
    "        temp = []\n",
    "        # iterate over given answer\n",
    "        # adjust here for different df structures!\n",
    "        for token in nlp(row[1][2]):\n",
    "            temp.append(token.lemma_)\n",
    "        # add new tags to complete list of tags\n",
    "        lemmas.append(temp)\n",
    "\n",
    "        # defines output of verbose run of function\n",
    "        if verbose:\n",
    "            print(\"Lemmatizing answer\", i+1, \"of\", end)\n",
    "\n",
    "        # stops function when limit is reached \n",
    "        if limit != -1:\n",
    "            if i == limit - 1: return(lemmas)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the POS tag function\n",
    "lemmalist = lemmatize(ld_q1, verbose=True, limit=10)\n",
    "print(ld_q1['Comments'][0])\n",
    "print(lemmalist[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
